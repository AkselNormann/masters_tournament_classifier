{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I attempt to use machine learning to predict whether or not a golfer will make the cut at the Masters Tournament. The models used take inputs consisting of full season aggregate statistics from the previous year to make the predictions for that year's Masters Tournament (e.g., the models used 2018 full season data to predict 2019 results.\n",
    "\n",
    "Additionally, we will use the subset of golfers that make the cut to attempt to predict the top 10 come Sunday evening at Augusta using regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
      "/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "# Webscrapping\n",
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "# Regression\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "# Classification\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Class imbalance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Dimesionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "\n",
    "# Graphing\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')\n",
    "figsize = (15,8)\n",
    "hspace = 0.5\n",
    "\n",
    "# Pickling\n",
    "import pickle\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PGATour.com Webscrapping Function\n",
    "def get_PGA_Tour_data(stats, seasons):\n",
    "\n",
    "    \"\"\"\n",
    "    This function pings the PGA Tour's server and gathers the desired statistics\n",
    "    (by inserting the stat_id into the url) and appends it to our data container\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "    \n",
    "    for season in seasons:  \n",
    "        print(f\"Beginning {season} season:\")\n",
    "        for stat in stats: \n",
    "            url = f\"https://www.pgatour.com/content/pgatour/stats/stat.{stat['stat_id']}.y{season}.html\"\n",
    "\n",
    "            #opening up connection, grabbing the page\n",
    "            uClient = uReq(url)\n",
    "            page_html = uClient.read()\n",
    "\n",
    "            #html parsing using BeautifulSoup\n",
    "            page_soup = soup(page_html, 'html.parser')\n",
    "\n",
    "            #find the table where stats are kept\n",
    "            tbody = page_soup.find('tbody')\n",
    "\n",
    "            #each golfer is separated by a <tr> tag\n",
    "            raw_golfers = tbody.findAll('tr')\n",
    "\n",
    "            #loop through each golfer, grab name and avg. distance\n",
    "            for raw_golfer in raw_golfers:\n",
    "                golfer = {}\n",
    "                golfer['season'] = season            \n",
    "                name = raw_golfer.find('td', {'class':'player-name'}).a.text\n",
    "                golfer['full_name'] = name.replace(' ', '_').lower()\n",
    "                golfer[f\"{stat['stat_name']}\"] = raw_golfer.find('td', {'class':None}).text\n",
    "                data.append(golfer)\n",
    "            print(f\" {stat['stat_name']} stats added for the {season} season\")\n",
    "        print(f\"{season} season completed.\\n\")\n",
    "\n",
    "    #close the client\n",
    "    uClient.close()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Strings to Feet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert stats to feet function\n",
    "def convert_to_feet(x):\n",
    "    \"\"\"\n",
    "    This function takes in a string and returns a float\n",
    "    \"\"\"\n",
    "    \n",
    "    if type(x) == str:\n",
    "        y = x.replace(\"'\",\"\").replace('\"',\"\").split()\n",
    "\n",
    "        if len(y) == 1:\n",
    "            y.extend([0])\n",
    "            for i in range(len(y)):\n",
    "                y[i] = int(y[i])\n",
    "            return y[0]+(y[1]/12)\n",
    "\n",
    "        else:\n",
    "            for i in range(len(y)):\n",
    "                y[i] = int(y[i])\n",
    "            return y[0]+(y[1]/12)\n",
    "\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Golfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format golfer names from results Dataframe\n",
    "def format_golfer(x):\n",
    "    return x.replace(' ','_').lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotter Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the confusion matrix\n",
    "def plot_confusion_matrix(cm, classes):\n",
    "    cmap = plt.cm.Greens\n",
    "    fig = plt.figure(figsize=(5,4));\n",
    "    ax = fig.add_subplot(111);\n",
    "    cax = ax.matshow(cm, cmap=cmap);\n",
    "    fig.colorbar(cax);\n",
    "    ax.set_xticklabels([''] + classes);\n",
    "    ax.set_yticklabels([''] + classes);\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'), {\n",
    "            'fontsize' : 18,\n",
    "            'horizontalalignment' : \"center\",\n",
    "            'verticalalignment' : \"center\"\n",
    "        }, color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.xlabel('Predicted');\n",
    "    plt.ylabel('Actual') ;\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates feature importance graphs\n",
    "def plot_feature_importances(X_train, model, n_features, FI_labels):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center', fc='yellow', ec='darkgreen') \n",
    "    plt.yticks(np.arange(n_features), FI_labels) \n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.ylabel('Feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Results Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression_results(ax, y_true, y_pred, title, scores, elapsed_time):\n",
    "    \"\"\"Scatter plot of the predicted vs true targets\"\"\"\n",
    "    ax.plot([y_true.min(), y_true.max()],\n",
    "            [y_true.min(), y_true.max()],\n",
    "            '--r', linewidth=2)\n",
    "    ax.scatter(y_true, y_pred, alpha=0.2)\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    ax.spines['left'].set_position(('outward', 10))\n",
    "    ax.spines['bottom'].set_position(('outward', 10))\n",
    "    ax.set_xlim([y_true.min(), y_true.max()])\n",
    "    ax.set_ylim([y_true.min(), y_true.max()])\n",
    "    ax.set_xlabel('Measured')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    extra = plt.Rectangle((0, 0), 0, 0, fc=\"w\", fill=False,\n",
    "                          edgecolor='none', linewidth=0)\n",
    "    ax.legend([extra], [scores], loc='upper left')\n",
    "    title = title + '\\n Evaluation in {:.2f} seconds'.format(elapsed_time)\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Gathering and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desired Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The stat_id will be inserted into the url for each ping\n",
    "# stats = [\n",
    "#     {'stat_name': 'driving_distance',\n",
    "#      'stat_id': '101'},\n",
    "#     {'stat_name': 'driving_accuracy', \n",
    "#      'stat_id': '102'},\n",
    "#     {'stat_name': 'greens_in_regulation', \n",
    "#      'stat_id': '103'},\n",
    "#     {'stat_name': 'proximity', \n",
    "#      'stat_id': '331'},\n",
    "#     {'stat_name': 'scrambling_rough',  \n",
    "#      'stat_id': '363'},\n",
    "#     {'stat_name': 'scrambling_sand',  \n",
    "#      'stat_id': '362'},\n",
    "#     {'stat_name': 'putting_conversion',\n",
    "#      'stat_id': '115'},\n",
    "#     {'stat_name': 'putting_rating',\n",
    "#      'stat_id': '402'},\n",
    "#     {'stat_name': 'sg_off_tee',\n",
    "#      'stat_id': '02567'},\n",
    "#     {'stat_name': 'sg_approach',\n",
    "#      'stat_id': '02568'},\n",
    "#     {'stat_name': 'sg_scrambling',  \n",
    "#      'stat_id': '02569'},\n",
    "#     {'stat_name': 'sg_putting',\n",
    "#      'stat_id': '02564'},\n",
    "#     {'stat_name': 'sg_tee_to_green',  \n",
    "#      'stat_id': '02674'},\n",
    "#     {'stat_name': 'sg_total',  \n",
    "#      'stat_id': '02675'},\n",
    "#     {'stat_name': 'scoring_avg',  \n",
    "#      'stat_id': '120'},\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasons Considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Since strokes_gained stats only go back to 2004, we will consider 2004-2019\n",
    "# seasons = range(2004, 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webscrapping and Dataframe Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Call the web scrapping helper function defined above. Save the results and examine Dataframe\n",
    "# data = get_PGA_Tour_data(stats, seasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert data to Dataframe and save raw data locally\n",
    "# df = pd.DataFrame(data)\n",
    "# df.to_csv('csv_files/golfer_data/raw_golfer_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48592, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>full_name</th>\n",
       "      <th>driving_distance</th>\n",
       "      <th>driving_accuracy</th>\n",
       "      <th>greens_in_regulation</th>\n",
       "      <th>proximity</th>\n",
       "      <th>scrambling_rough</th>\n",
       "      <th>scrambling_sand</th>\n",
       "      <th>putting_conversion</th>\n",
       "      <th>putting_rating</th>\n",
       "      <th>sg_off_tee</th>\n",
       "      <th>sg_approach</th>\n",
       "      <th>sg_scrambling</th>\n",
       "      <th>sg_putting</th>\n",
       "      <th>sg_tee_to_green</th>\n",
       "      <th>sg_total</th>\n",
       "      <th>scoring_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2004</td>\n",
       "      <td>hank_kuehne</td>\n",
       "      <td>314.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2004</td>\n",
       "      <td>scott_hend</td>\n",
       "      <td>312.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2004</td>\n",
       "      <td>john_daly</td>\n",
       "      <td>306.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2004</td>\n",
       "      <td>mike_heinen</td>\n",
       "      <td>305.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2004</td>\n",
       "      <td>chris_smith</td>\n",
       "      <td>304.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   season    full_name  driving_distance  driving_accuracy  \\\n",
       "0    2004  hank_kuehne             314.4               NaN   \n",
       "1    2004   scott_hend             312.6               NaN   \n",
       "2    2004    john_daly             306.0               NaN   \n",
       "3    2004  mike_heinen             305.2               NaN   \n",
       "4    2004  chris_smith             304.0               NaN   \n",
       "\n",
       "   greens_in_regulation proximity  scrambling_rough  scrambling_sand  \\\n",
       "0                   NaN       NaN               NaN              NaN   \n",
       "1                   NaN       NaN               NaN              NaN   \n",
       "2                   NaN       NaN               NaN              NaN   \n",
       "3                   NaN       NaN               NaN              NaN   \n",
       "4                   NaN       NaN               NaN              NaN   \n",
       "\n",
       "   putting_conversion  putting_rating  sg_off_tee  sg_approach  sg_scrambling  \\\n",
       "0                 NaN             NaN         NaN          NaN            NaN   \n",
       "1                 NaN             NaN         NaN          NaN            NaN   \n",
       "2                 NaN             NaN         NaN          NaN            NaN   \n",
       "3                 NaN             NaN         NaN          NaN            NaN   \n",
       "4                 NaN             NaN         NaN          NaN            NaN   \n",
       "\n",
       "   sg_putting  sg_tee_to_green  sg_total  scoring_avg  \n",
       "0         NaN              NaN       NaN          NaN  \n",
       "1         NaN              NaN       NaN          NaN  \n",
       "2         NaN              NaN       NaN          NaN  \n",
       "3         NaN              NaN       NaN          NaN  \n",
       "4         NaN              NaN       NaN          NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload raw data and examine dataframe\n",
    "df = pd.read_csv('csv_files/golfer_data/raw_golfer_data.csv', index_col=0)\n",
    "df.drop('str_diff_to_field', axis=1, inplace=True)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert proximity to feet floats and str_diff_to_field to stroke floats\n",
    "df['proximity'] = df['proximity'].apply(lambda x: convert_to_feet(x))\n",
    "\n",
    "# Convert percentage features to decimals\n",
    "df['driving_accuracy'] = df['driving_accuracy']/100\n",
    "df['greens_in_regulation'] = df['greens_in_regulation']/100\n",
    "df['scrambling_rough'] = df['scrambling_rough']/100\n",
    "df['scrambling_sand'] = df['scrambling_sand']/100\n",
    "df['putting_conversion'] = df['putting_conversion']/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby Season and Golfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3036, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>full_name</th>\n",
       "      <th>driving_distance</th>\n",
       "      <th>driving_accuracy</th>\n",
       "      <th>greens_in_regulation</th>\n",
       "      <th>proximity</th>\n",
       "      <th>scrambling_rough</th>\n",
       "      <th>scrambling_sand</th>\n",
       "      <th>putting_conversion</th>\n",
       "      <th>putting_rating</th>\n",
       "      <th>sg_off_tee</th>\n",
       "      <th>sg_approach</th>\n",
       "      <th>sg_scrambling</th>\n",
       "      <th>sg_putting</th>\n",
       "      <th>sg_tee_to_green</th>\n",
       "      <th>sg_total</th>\n",
       "      <th>scoring_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2004</td>\n",
       "      <td>aaron_baddeley</td>\n",
       "      <td>288.0</td>\n",
       "      <td>0.5308</td>\n",
       "      <td>0.5817</td>\n",
       "      <td>39.333333</td>\n",
       "      <td>0.5769</td>\n",
       "      <td>0.5351</td>\n",
       "      <td>0.3015</td>\n",
       "      <td>1.576</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.679</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.579</td>\n",
       "      <td>-1.008</td>\n",
       "      <td>-0.429</td>\n",
       "      <td>71.614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2004</td>\n",
       "      <td>adam_scott</td>\n",
       "      <td>295.4</td>\n",
       "      <td>0.5765</td>\n",
       "      <td>0.6560</td>\n",
       "      <td>35.333333</td>\n",
       "      <td>0.5301</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.3290</td>\n",
       "      <td>1.611</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.571</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.603</td>\n",
       "      <td>1.427</td>\n",
       "      <td>70.096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2004</td>\n",
       "      <td>alex_cejka</td>\n",
       "      <td>285.8</td>\n",
       "      <td>0.6421</td>\n",
       "      <td>0.6381</td>\n",
       "      <td>36.083333</td>\n",
       "      <td>0.5108</td>\n",
       "      <td>0.5765</td>\n",
       "      <td>0.2877</td>\n",
       "      <td>1.625</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.388</td>\n",
       "      <td>71.153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2004</td>\n",
       "      <td>andre_stolz</td>\n",
       "      <td>297.9</td>\n",
       "      <td>0.5897</td>\n",
       "      <td>0.6300</td>\n",
       "      <td>36.416667</td>\n",
       "      <td>0.4632</td>\n",
       "      <td>0.5244</td>\n",
       "      <td>0.2838</td>\n",
       "      <td>1.628</td>\n",
       "      <td>-0.333</td>\n",
       "      <td>-0.532</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>-1.002</td>\n",
       "      <td>-1.247</td>\n",
       "      <td>72.341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2004</td>\n",
       "      <td>arjun_atwal</td>\n",
       "      <td>289.4</td>\n",
       "      <td>0.6048</td>\n",
       "      <td>0.6252</td>\n",
       "      <td>35.916667</td>\n",
       "      <td>0.5979</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2980</td>\n",
       "      <td>1.606</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>71.688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   season       full_name  driving_distance  driving_accuracy  \\\n",
       "0    2004  aaron_baddeley             288.0            0.5308   \n",
       "1    2004      adam_scott             295.4            0.5765   \n",
       "2    2004      alex_cejka             285.8            0.6421   \n",
       "3    2004     andre_stolz             297.9            0.5897   \n",
       "4    2004     arjun_atwal             289.4            0.6048   \n",
       "\n",
       "   greens_in_regulation  proximity  scrambling_rough  scrambling_sand  \\\n",
       "0                0.5817  39.333333            0.5769           0.5351   \n",
       "1                0.6560  35.333333            0.5301           0.6170   \n",
       "2                0.6381  36.083333            0.5108           0.5765   \n",
       "3                0.6300  36.416667            0.4632           0.5244   \n",
       "4                0.6252  35.916667            0.5979           0.4107   \n",
       "\n",
       "   putting_conversion  putting_rating  sg_off_tee  sg_approach  sg_scrambling  \\\n",
       "0              0.3015           1.576      -0.530       -0.679          0.201   \n",
       "1              0.3290           1.611       0.180        0.571         -0.147   \n",
       "2              0.2877           1.625       0.119        0.255          0.020   \n",
       "3              0.2838           1.628      -0.333       -0.532         -0.137   \n",
       "4              0.2980           1.606       0.013       -0.097         -0.116   \n",
       "\n",
       "   sg_putting  sg_tee_to_green  sg_total  scoring_avg  \n",
       "0       0.579           -1.008    -0.429       71.614  \n",
       "1       0.824            0.603     1.427       70.096  \n",
       "2      -0.006            0.394     0.388       71.153  \n",
       "3      -0.246           -1.002    -1.247       72.341  \n",
       "4      -0.034           -0.200    -0.234       71.688  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group stats by season and golfer\n",
    "df = df.groupby(['season', 'full_name'])[\n",
    "    'driving_distance','driving_accuracy','greens_in_regulation','proximity',\n",
    "    'scrambling_rough','scrambling_sand','putting_conversion','putting_rating',\n",
    "    'sg_off_tee','sg_approach','sg_scrambling','sg_putting','sg_tee_to_green',\n",
    "    'sg_total','scoring_avg'].mean()\n",
    "\n",
    "# Save grouped data locally\n",
    "df.to_csv('csv_files/golfer_data/golfer_data.csv')\n",
    "\n",
    "# Reload and examine data\n",
    "df = pd.read_csv('csv_files/golfer_data/golfer_data.csv')\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Missing Golfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>full_name</th>\n",
       "      <th>driving_distance</th>\n",
       "      <th>driving_accuracy</th>\n",
       "      <th>greens_in_regulation</th>\n",
       "      <th>proximity</th>\n",
       "      <th>scrambling_rough</th>\n",
       "      <th>scrambling_sand</th>\n",
       "      <th>putting_conversion</th>\n",
       "      <th>putting_rating</th>\n",
       "      <th>sg_off_tee</th>\n",
       "      <th>sg_approach</th>\n",
       "      <th>sg_scrambling</th>\n",
       "      <th>sg_putting</th>\n",
       "      <th>sg_tee_to_green</th>\n",
       "      <th>sg_total</th>\n",
       "      <th>scoring_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2004</td>\n",
       "      <td>trevor_immelman</td>\n",
       "      <td>282.3</td>\n",
       "      <td>63.89</td>\n",
       "      <td>60.34</td>\n",
       "      <td>36' 5\"</td>\n",
       "      <td>50.85</td>\n",
       "      <td>48.48</td>\n",
       "      <td>25.45</td>\n",
       "      <td>1.633</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.282</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-1.020</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.207</td>\n",
       "      <td>71.224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2005</td>\n",
       "      <td>ángel_cabrera</td>\n",
       "      <td>311.2</td>\n",
       "      <td>53.30</td>\n",
       "      <td>63.33</td>\n",
       "      <td>37' 10\"</td>\n",
       "      <td>54.29</td>\n",
       "      <td>47.50</td>\n",
       "      <td>29.03</td>\n",
       "      <td>1.665</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.141</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.603</td>\n",
       "      <td>70.669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>paul_casey</td>\n",
       "      <td>305.3</td>\n",
       "      <td>62.66</td>\n",
       "      <td>67.17</td>\n",
       "      <td>32' 1\"</td>\n",
       "      <td>45.00</td>\n",
       "      <td>53.57</td>\n",
       "      <td>26.42</td>\n",
       "      <td>1.707</td>\n",
       "      <td>0.118</td>\n",
       "      <td>1.385</td>\n",
       "      <td>-0.249</td>\n",
       "      <td>-0.641</td>\n",
       "      <td>1.254</td>\n",
       "      <td>0.613</td>\n",
       "      <td>70.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2007</td>\n",
       "      <td>andres_romero</td>\n",
       "      <td>312.5</td>\n",
       "      <td>56.25</td>\n",
       "      <td>58.89</td>\n",
       "      <td>33' 10\"</td>\n",
       "      <td>76.47</td>\n",
       "      <td>62.16</td>\n",
       "      <td>26.42</td>\n",
       "      <td>1.572</td>\n",
       "      <td>1.988</td>\n",
       "      <td>-0.457</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.416</td>\n",
       "      <td>1.873</td>\n",
       "      <td>2.289</td>\n",
       "      <td>68.653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2007</td>\n",
       "      <td>robert_karlsson</td>\n",
       "      <td>298.9</td>\n",
       "      <td>51.02</td>\n",
       "      <td>57.54</td>\n",
       "      <td>36' 2\"</td>\n",
       "      <td>51.55</td>\n",
       "      <td>44.44</td>\n",
       "      <td>27.36</td>\n",
       "      <td>1.631</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>71.396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   season        full_name  driving_distance  driving_accuracy  \\\n",
       "0    2004  trevor_immelman             282.3             63.89   \n",
       "1    2005    ángel_cabrera             311.2             53.30   \n",
       "2    2006       paul_casey             305.3             62.66   \n",
       "3    2007    andres_romero             312.5             56.25   \n",
       "4    2007  robert_karlsson             298.9             51.02   \n",
       "\n",
       "   greens_in_regulation proximity  scrambling_rough  scrambling_sand  \\\n",
       "0                 60.34    36' 5\"             50.85            48.48   \n",
       "1                 63.33   37' 10\"             54.29            47.50   \n",
       "2                 67.17    32' 1\"             45.00            53.57   \n",
       "3                 58.89   33' 10\"             76.47            62.16   \n",
       "4                 57.54    36' 2\"             51.55            44.44   \n",
       "\n",
       "   putting_conversion  putting_rating  sg_off_tee  sg_approach  sg_scrambling  \\\n",
       "0               25.45           1.633       0.057        0.282         -0.031   \n",
       "1               29.03           1.665       0.531        0.067          0.141   \n",
       "2               26.42           1.707       0.118        1.385         -0.249   \n",
       "3               26.42           1.572       1.988       -0.457          0.342   \n",
       "4               27.36           1.631      -0.130       -0.219         -0.025   \n",
       "\n",
       "   sg_putting  sg_tee_to_green  sg_total  scoring_avg  \n",
       "0      -1.020            0.308     0.207       71.224  \n",
       "1      -0.136            0.739     0.603       70.669  \n",
       "2      -0.641            1.254     0.613       70.837  \n",
       "3       0.416            1.873     2.289       68.653  \n",
       "4      -0.374           -0.022    -0.396       71.396  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import new csv file that contains data on missing golfers\n",
    "missing = pd.read_csv('csv_files/masters/masters_results - missing.csv')\n",
    "\n",
    "print(missing.shape)\n",
    "missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the features of missing golfers\n",
    "missing['proximity'] = missing['proximity'].apply(lambda x: convert_to_feet(x))\n",
    "\n",
    "# Convert percentage features to decimals\n",
    "missing['driving_accuracy'] = missing['driving_accuracy']/100\n",
    "missing['greens_in_regulation'] = missing['greens_in_regulation']/100\n",
    "missing['scrambling_rough'] = missing['scrambling_rough']/100\n",
    "missing['scrambling_sand'] = missing['scrambling_sand']/100\n",
    "missing['putting_conversion'] = missing['putting_conversion']/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = [df, missing]\n",
    "df = pd.concat(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3063, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>full_name</th>\n",
       "      <th>driving_distance</th>\n",
       "      <th>driving_accuracy</th>\n",
       "      <th>greens_in_regulation</th>\n",
       "      <th>proximity</th>\n",
       "      <th>scrambling_rough</th>\n",
       "      <th>scrambling_sand</th>\n",
       "      <th>putting_conversion</th>\n",
       "      <th>putting_rating</th>\n",
       "      <th>sg_off_tee</th>\n",
       "      <th>sg_approach</th>\n",
       "      <th>sg_scrambling</th>\n",
       "      <th>sg_putting</th>\n",
       "      <th>sg_tee_to_green</th>\n",
       "      <th>sg_total</th>\n",
       "      <th>scoring_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2004</td>\n",
       "      <td>aaron_baddeley</td>\n",
       "      <td>288.0</td>\n",
       "      <td>0.5308</td>\n",
       "      <td>0.5817</td>\n",
       "      <td>39.333333</td>\n",
       "      <td>0.5769</td>\n",
       "      <td>0.5351</td>\n",
       "      <td>0.3015</td>\n",
       "      <td>1.576</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.679</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.579</td>\n",
       "      <td>-1.008</td>\n",
       "      <td>-0.429</td>\n",
       "      <td>71.614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2004</td>\n",
       "      <td>adam_scott</td>\n",
       "      <td>295.4</td>\n",
       "      <td>0.5765</td>\n",
       "      <td>0.6560</td>\n",
       "      <td>35.333333</td>\n",
       "      <td>0.5301</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.3290</td>\n",
       "      <td>1.611</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.571</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.603</td>\n",
       "      <td>1.427</td>\n",
       "      <td>70.096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2004</td>\n",
       "      <td>alex_cejka</td>\n",
       "      <td>285.8</td>\n",
       "      <td>0.6421</td>\n",
       "      <td>0.6381</td>\n",
       "      <td>36.083333</td>\n",
       "      <td>0.5108</td>\n",
       "      <td>0.5765</td>\n",
       "      <td>0.2877</td>\n",
       "      <td>1.625</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.388</td>\n",
       "      <td>71.153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2004</td>\n",
       "      <td>andre_stolz</td>\n",
       "      <td>297.9</td>\n",
       "      <td>0.5897</td>\n",
       "      <td>0.6300</td>\n",
       "      <td>36.416667</td>\n",
       "      <td>0.4632</td>\n",
       "      <td>0.5244</td>\n",
       "      <td>0.2838</td>\n",
       "      <td>1.628</td>\n",
       "      <td>-0.333</td>\n",
       "      <td>-0.532</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>-1.002</td>\n",
       "      <td>-1.247</td>\n",
       "      <td>72.341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2004</td>\n",
       "      <td>arjun_atwal</td>\n",
       "      <td>289.4</td>\n",
       "      <td>0.6048</td>\n",
       "      <td>0.6252</td>\n",
       "      <td>35.916667</td>\n",
       "      <td>0.5979</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2980</td>\n",
       "      <td>1.606</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>71.688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   season       full_name  driving_distance  driving_accuracy  \\\n",
       "0    2004  aaron_baddeley             288.0            0.5308   \n",
       "1    2004      adam_scott             295.4            0.5765   \n",
       "2    2004      alex_cejka             285.8            0.6421   \n",
       "3    2004     andre_stolz             297.9            0.5897   \n",
       "4    2004     arjun_atwal             289.4            0.6048   \n",
       "\n",
       "   greens_in_regulation  proximity  scrambling_rough  scrambling_sand  \\\n",
       "0                0.5817  39.333333            0.5769           0.5351   \n",
       "1                0.6560  35.333333            0.5301           0.6170   \n",
       "2                0.6381  36.083333            0.5108           0.5765   \n",
       "3                0.6300  36.416667            0.4632           0.5244   \n",
       "4                0.6252  35.916667            0.5979           0.4107   \n",
       "\n",
       "   putting_conversion  putting_rating  sg_off_tee  sg_approach  sg_scrambling  \\\n",
       "0              0.3015           1.576      -0.530       -0.679          0.201   \n",
       "1              0.3290           1.611       0.180        0.571         -0.147   \n",
       "2              0.2877           1.625       0.119        0.255          0.020   \n",
       "3              0.2838           1.628      -0.333       -0.532         -0.137   \n",
       "4              0.2980           1.606       0.013       -0.097         -0.116   \n",
       "\n",
       "   sg_putting  sg_tee_to_green  sg_total  scoring_avg  \n",
       "0       0.579           -1.008    -0.429       71.614  \n",
       "1       0.824            0.603     1.427       70.096  \n",
       "2      -0.006            0.394     0.388       71.153  \n",
       "3      -0.246           -1.002    -1.247       72.341  \n",
       "4      -0.034           -0.200    -0.234       71.688  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=['season', 'full_name'], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporate Historical Tournament Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1469, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>masters_year</th>\n",
       "      <th>full_name</th>\n",
       "      <th>total_score</th>\n",
       "      <th>prior_score</th>\n",
       "      <th>roll_5yr_majors</th>\n",
       "      <th>roll_3yr_majors</th>\n",
       "      <th>roll_1yr_majors</th>\n",
       "      <th>made_cut</th>\n",
       "      <th>top_10</th>\n",
       "      <th>champion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2005</td>\n",
       "      <td>adam_scott</td>\n",
       "      <td>294</td>\n",
       "      <td>315</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2005</td>\n",
       "      <td>austin_eaton_iii</td>\n",
       "      <td>315</td>\n",
       "      <td>315</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2005</td>\n",
       "      <td>ben_crenshaw</td>\n",
       "      <td>315</td>\n",
       "      <td>315</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2005</td>\n",
       "      <td>ben_curtis</td>\n",
       "      <td>315</td>\n",
       "      <td>315</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2005</td>\n",
       "      <td>bernhard_langer</td>\n",
       "      <td>289</td>\n",
       "      <td>285</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   masters_year         full_name  total_score  prior_score  roll_5yr_majors  \\\n",
       "0          2005        adam_scott          294          315                0   \n",
       "2          2005  austin_eaton_iii          315          315                0   \n",
       "3          2005      ben_crenshaw          315          315                0   \n",
       "4          2005        ben_curtis          315          315                1   \n",
       "5          2005   bernhard_langer          289          285                0   \n",
       "\n",
       "   roll_3yr_majors  roll_1yr_majors  made_cut  top_10  champion  \n",
       "0                0                0         1       0         0  \n",
       "2                0                0         0       0         0  \n",
       "3                0                0         0       0         0  \n",
       "4                1                0         0       0         0  \n",
       "5                0                0         1       0         0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Masters Results data and format each golfer to match our data's format\n",
    "masters_results = pd.read_csv('csv_files/masters/masters_results - data.csv')\n",
    "masters_results['full_name'] = masters_results['full_name'].apply(lambda x: format_golfer(x))\n",
    "masters_results = masters_results.sort_values(by=['masters_year', 'full_name'])\n",
    "\n",
    "print(masters_results.shape)\n",
    "masters_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataframes together on name\n",
    "df = pd.merge(df, masters_results, how='left', on='full_name')\n",
    "\n",
    "# Drop Nans\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Only consider rows where golfer participated the following masters\n",
    "df = df[df['masters_year']-df['season'] == 1]\n",
    "\n",
    "# Drop Masters year and reset Index\n",
    "df.drop('masters_year', axis=1, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Change floats to integers\n",
    "df['made_cut'] = df['made_cut'].astype(int)\n",
    "df['top_10'] = df['top_10'].astype(int)\n",
    "df['champion'] = df['champion'].astype(int)\n",
    "df['total_score'] = df['total_score'].astype(int)\n",
    "df['prior_score'] = df['prior_score'].astype(int)\n",
    "df['roll_5yr_majors'] = df['roll_5yr_majors'].astype(int)\n",
    "df['roll_3yr_majors'] = df['roll_3yr_majors'].astype(int)\n",
    "df['roll_1yr_majors'] = df['roll_1yr_majors'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Majors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['exp_5'] = np.exp(df['roll_5yr_majors']*1)\n",
    "df['exp_3'] = np.exp(df['roll_3yr_majors']*5)\n",
    "df['exp_1'] = np.exp(df['roll_1yr_majors']*10)\n",
    "df['rolling_majors'] = df['exp_5']*df['exp_3']*df['exp_1']\n",
    "df['rolling_majors'] = np.log(df['rolling_majors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Birdie or Better Distance\n",
    "Generate a circle where the radius equals a golfer's maximum distance from the hole in which the golfer will make a birdie or better from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['otg_area'] = df['proximity']**2 * np.pi\n",
    "df['bob_area'] = df['otg_area']*df['putting_conversion']\n",
    "df['bob_distance'] = np.sqrt(df['bob_area']/np.pi)\n",
    "df.drop(['proximity', 'putting_conversion',\n",
    "         'otg_area', 'bob_area'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field Average Score\n",
    "Create a column that is the average score of the field that year for each golfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reorder dataframe so that it is sorted by golfer and than seaon\n",
    "df = df.groupby(['full_name', 'season'])['champion', 'top_10', 'made_cut', 'total_score', 'prior_score',\n",
    "        'roll_5yr_majors', 'roll_3yr_majors', 'roll_1yr_majors', 'rolling_majors',\n",
    "        'driving_distance', 'driving_accuracy', 'greens_in_regulation', 'bob_distance',\n",
    "        'scrambling_rough', 'scrambling_sand', 'putting_rating', 'scoring_avg', \n",
    "        'sg_off_tee', 'sg_approach', 'sg_scrambling', 'sg_putting', 'sg_tee_to_green', 'sg_total'].mean()\n",
    "\n",
    "# Save and reload csv\n",
    "df.to_csv('csv_files/golfer_data/golfer_data.csv')\n",
    "\n",
    "# Reload and examine dataframe\n",
    "df = pd.read_csv('csv_files/golfer_data/golfer_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get each year's Masters Tournament average score for the field\n",
    "hist_scores = pd.DataFrame(df.groupby('season')['prior_score'].mean())\n",
    "hist_scores.reset_index(drop=False, inplace=True)\n",
    "hist_scores['prior_afs'] = hist_scores['prior_score'].astype(int)\n",
    "hist_scores.drop('prior_score', axis=1, inplace=True)\n",
    "\n",
    "# Merge with our dataframe\n",
    "df = pd.merge(df, hist_scores, how='inner', on='season')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strokes Gained On Field\n",
    "Create a new column that calculates how much better (or worse) the golfer did than the field average that year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column\n",
    "df['sg_on_field'] = df['prior_afs']-df['prior_score']\n",
    "\n",
    "# Reorder dataframe so that it is sorted by golfer and than seaon\n",
    "df = df.groupby(['full_name', 'season'])['champion', 'top_10', 'made_cut',\n",
    "        'total_score', 'prior_score', 'prior_afs', 'sg_on_field',\n",
    "        'roll_5yr_majors', 'roll_3yr_majors', 'roll_1yr_majors', 'rolling_majors',\n",
    "        'driving_distance', 'driving_accuracy', 'greens_in_regulation', 'bob_distance',\n",
    "        'scrambling_rough', 'scrambling_sand', 'putting_rating', 'scoring_avg',\n",
    "        'sg_off_tee', 'sg_approach', 'sg_scrambling', 'sg_putting', 'sg_tee_to_green', 'sg_total'].mean()\n",
    "\n",
    "# Save and reload csv\n",
    "df.to_csv('csv_files/golfer_data/golfer_data.csv')\n",
    "\n",
    "# Reload and examine dataframe\n",
    "df = pd.read_csv('csv_files/golfer_data/golfer_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience and Rolling Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of unique golfers and nan values to poulate 2019\n",
    "golfers = df['full_name'].unique().tolist()\n",
    "\n",
    "# Create a list of dictionaries to house each golfers prior score at the Masters for every year\n",
    "names_and_scores = []\n",
    "for golfer in golfers:\n",
    "    name_and_scores = {}\n",
    "    name_and_scores['golfer'] = golfer\n",
    "    prior_scores = []\n",
    "    strokes_gained_on_field = []\n",
    "    for i in range(df.shape[0]):\n",
    "        if golfer == df['full_name'].iloc[i]:\n",
    "            prior_scores.append(df['prior_score'].iloc[i])\n",
    "            strokes_gained_on_field.append(df['sg_on_field'].iloc[i])\n",
    "        else:\n",
    "            continue\n",
    "    name_and_scores['prior_scores'] = prior_scores\n",
    "    name_and_scores['strokes_gained_on_field'] = strokes_gained_on_field\n",
    "    names_and_scores.append(name_and_scores)\n",
    "\n",
    "# Loop through list of dictionaries and create a rolling avg of prior scores list for each golfer\n",
    "for golfer in names_and_scores:\n",
    "    prior_scores_rolling_avg = []\n",
    "    strokes_gained_rolling_avg = []\n",
    "    experiences = []\n",
    "    i = 1\n",
    "    while i <= len(golfer['prior_scores']):\n",
    "        prior_score_rolling_avg = np.mean(golfer['prior_scores'][:i])\n",
    "        prior_scores_rolling_avg.append(prior_score_rolling_avg)\n",
    "        sg_rolling_avg = np.mean(golfer['strokes_gained_on_field'][:i])\n",
    "        strokes_gained_rolling_avg.append(sg_rolling_avg)\n",
    "        experience = len(prior_scores_rolling_avg)\n",
    "        experiences.append(experience)\n",
    "        i += 1\n",
    "    golfer['ps_roll_avg'] = prior_scores_rolling_avg\n",
    "    golfer['sg_roll_avg'] = strokes_gained_rolling_avg\n",
    "    golfer['experience'] = experiences\n",
    "\n",
    "# Join all of the sepearate lists together\n",
    "prior_scores_rolling_avg = []\n",
    "strokes_gained_rolling_avg = []\n",
    "experiences = []\n",
    "for golfer in names_and_scores:\n",
    "    prior_scores_rolling_avg.extend(golfer['ps_roll_avg'])\n",
    "    strokes_gained_rolling_avg.extend(golfer['sg_roll_avg'])\n",
    "    experiences.extend(golfer['experience'])\n",
    "\n",
    "# Add to our dataframe\n",
    "df['experience'] = experiences\n",
    "df['ps_roll_avg'] = prior_scores_rolling_avg\n",
    "df['sg_roll_avg'] = strokes_gained_rolling_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strokes Gained Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(951, 22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_name</th>\n",
       "      <th>season</th>\n",
       "      <th>experience</th>\n",
       "      <th>champion</th>\n",
       "      <th>top_10</th>\n",
       "      <th>made_cut</th>\n",
       "      <th>total_score</th>\n",
       "      <th>prior_score</th>\n",
       "      <th>ps_roll_avg</th>\n",
       "      <th>prior_afs</th>\n",
       "      <th>...</th>\n",
       "      <th>rolling_majors</th>\n",
       "      <th>driving_distance</th>\n",
       "      <th>driving_accuracy</th>\n",
       "      <th>greens_in_regulation</th>\n",
       "      <th>bob_distance</th>\n",
       "      <th>scrambling_rough</th>\n",
       "      <th>scrambling_sand</th>\n",
       "      <th>putting_rating</th>\n",
       "      <th>scoring_avg</th>\n",
       "      <th>sg_stat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>aaron_baddeley</td>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>315</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>288.3</td>\n",
       "      <td>0.6073</td>\n",
       "      <td>0.5826</td>\n",
       "      <td>21.326667</td>\n",
       "      <td>0.6100</td>\n",
       "      <td>0.5271</td>\n",
       "      <td>1.553</td>\n",
       "      <td>71.197</td>\n",
       "      <td>0.999697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>aaron_baddeley</td>\n",
       "      <td>2007</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315</td>\n",
       "      <td>307</td>\n",
       "      <td>311.000000</td>\n",
       "      <td>305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>291.9</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6035</td>\n",
       "      <td>21.005830</td>\n",
       "      <td>0.6021</td>\n",
       "      <td>0.5905</td>\n",
       "      <td>1.570</td>\n",
       "      <td>70.088</td>\n",
       "      <td>0.994052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>aaron_baddeley</td>\n",
       "      <td>2008</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>284</td>\n",
       "      <td>315</td>\n",
       "      <td>312.333333</td>\n",
       "      <td>302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290.3</td>\n",
       "      <td>0.5945</td>\n",
       "      <td>0.6202</td>\n",
       "      <td>21.109307</td>\n",
       "      <td>0.5549</td>\n",
       "      <td>0.5694</td>\n",
       "      <td>1.577</td>\n",
       "      <td>70.196</td>\n",
       "      <td>0.999667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>aaron_baddeley</td>\n",
       "      <td>2010</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>293</td>\n",
       "      <td>315</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>298.9</td>\n",
       "      <td>0.5665</td>\n",
       "      <td>0.6460</td>\n",
       "      <td>21.237521</td>\n",
       "      <td>0.5458</td>\n",
       "      <td>0.5766</td>\n",
       "      <td>1.587</td>\n",
       "      <td>70.995</td>\n",
       "      <td>1.001072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>aaron_baddeley</td>\n",
       "      <td>2011</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>293</td>\n",
       "      <td>293</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>296.2</td>\n",
       "      <td>0.5567</td>\n",
       "      <td>0.6548</td>\n",
       "      <td>21.490982</td>\n",
       "      <td>0.4932</td>\n",
       "      <td>0.5673</td>\n",
       "      <td>1.536</td>\n",
       "      <td>70.230</td>\n",
       "      <td>1.005012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        full_name  season  experience  champion  top_10  made_cut  \\\n",
       "0  aaron_baddeley    2006           1         0       0         1   \n",
       "1  aaron_baddeley    2007           2         0       0         0   \n",
       "2  aaron_baddeley    2008           3         0       0         1   \n",
       "3  aaron_baddeley    2010           4         0       0         1   \n",
       "4  aaron_baddeley    2011           5         0       0         1   \n",
       "\n",
       "   total_score  prior_score  ps_roll_avg  prior_afs  ...  rolling_majors  \\\n",
       "0          307          315   315.000000        302  ...             0.0   \n",
       "1          315          307   311.000000        305  ...             0.0   \n",
       "2          284          315   312.333333        302  ...             0.0   \n",
       "3          293          315   313.000000        300  ...             0.0   \n",
       "4          293          293   309.000000        299  ...             0.0   \n",
       "\n",
       "   driving_distance  driving_accuracy  greens_in_regulation  bob_distance  \\\n",
       "0             288.3            0.6073                0.5826     21.326667   \n",
       "1             291.9            0.6000                0.6035     21.005830   \n",
       "2             290.3            0.5945                0.6202     21.109307   \n",
       "3             298.9            0.5665                0.6460     21.237521   \n",
       "4             296.2            0.5567                0.6548     21.490982   \n",
       "\n",
       "   scrambling_rough  scrambling_sand  putting_rating  scoring_avg   sg_stat  \n",
       "0            0.6100           0.5271           1.553       71.197  0.999697  \n",
       "1            0.6021           0.5905           1.570       70.088  0.994052  \n",
       "2            0.5549           0.5694           1.577       70.196  0.999667  \n",
       "3            0.5458           0.5766           1.587       70.995  1.001072  \n",
       "4            0.4932           0.5673           1.536       70.230  1.005012  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create feature to account for relationship between sg stats\n",
    "df['sg_stat'] = np.exp(df['sg_off_tee']*df['sg_approach']*df['sg_scrambling']*\n",
    "                    df['sg_putting']*df['sg_tee_to_green']*df['sg_total'])\n",
    "                \n",
    "# Account for sign (+/-)\n",
    "for i in range(df.shape[0]):\n",
    "    if df['sg_total'].iloc[i] < 0:\n",
    "        df['sg_stat'].iloc[i] = df['sg_stat'].iloc[i]*-1\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# Reorder and examine dataframe\n",
    "cols = ['full_name', 'season', 'experience', 'champion', 'top_10', 'made_cut',\n",
    "        'total_score', 'prior_score', 'ps_roll_avg', 'prior_afs', 'sg_on_field',\n",
    "        'sg_roll_avg', 'rolling_majors',\n",
    "        'driving_distance', 'driving_accuracy', 'greens_in_regulation', 'bob_distance',\n",
    "        'scrambling_rough', 'scrambling_sand', 'putting_rating', 'scoring_avg', 'sg_stat']\n",
    "\n",
    "df = df[cols]\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holdout Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout Set: (57, 22)\n",
      "Testing Data: (304, 22)\n"
     ]
    }
   ],
   "source": [
    "# Create holdout and testing datasets for 2009 data\n",
    "holdout_2009 = df[df['season'] == 2009]\n",
    "test_df_2009 = df[(df['season'] < 2009)]\n",
    "\n",
    "# Save both the holdout and validation set locally\n",
    "holdout_2009.to_csv('csv_files/holdouts/2009/holdout_data.csv')\n",
    "test_df_2009.to_csv('csv_files/holdouts/2009/test_df_data.csv')\n",
    "\n",
    "# Examine sizes\n",
    "print(f'Holdout Set: {holdout_2009.shape}')\n",
    "print(f'Testing Data: {test_df_2009.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout Set: (62, 22)\n",
      "Testing Data: (361, 22)\n"
     ]
    }
   ],
   "source": [
    "# Create holdout and testing datasets for 2010 data\n",
    "holdout_2010 = df[df['season'] == 2010]\n",
    "test_df_2010 = df[(df['season'] < 2010)]\n",
    "\n",
    "# Save both the holdout and validation set locally\n",
    "holdout_2010.to_csv('csv_files/holdouts/2010/holdout_data.csv')\n",
    "test_df_2010.to_csv('csv_files/holdouts/2010/test_df_data.csv')\n",
    "\n",
    "# Examine sizes\n",
    "print(f'Holdout Set: {holdout_2010.shape}')\n",
    "print(f'Testing Data: {test_df_2010.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout Set: (59, 22)\n",
      "Testing Data: (423, 22)\n"
     ]
    }
   ],
   "source": [
    "# Create holdout and testing datasets for 2011 data\n",
    "holdout_2011 = df[df['season'] == 2011]\n",
    "test_df_2011 = df[(df['season'] < 2011)]\n",
    "\n",
    "# Save both the holdout and validation set locally\n",
    "holdout_2011.to_csv('csv_files/holdouts/2011/holdout_data.csv')\n",
    "test_df_2011.to_csv('csv_files/holdouts/2011/test_df_data.csv')\n",
    "\n",
    "# Examine sizes\n",
    "print(f'Holdout Set: {holdout_2011.shape}')\n",
    "print(f'Testing Data: {test_df_2011.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout Set: (57, 22)\n",
      "Testing Data: (482, 22)\n"
     ]
    }
   ],
   "source": [
    "# Create holdout and testing datasets for 2012 data\n",
    "holdout_2012 = df[df['season'] == 2012]\n",
    "test_df_2012 = df[(df['season'] < 2012)]\n",
    "\n",
    "# Save both the holdout and validation set locally\n",
    "holdout_2012.to_csv('csv_files/holdouts/2012/holdout_data.csv')\n",
    "test_df_2012.to_csv('csv_files/holdouts/2012/test_df_data.csv')\n",
    "\n",
    "# Examine sizes\n",
    "print(f'Holdout Set: {holdout_2012.shape}')\n",
    "print(f'Testing Data: {test_df_2012.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout Set: (66, 22)\n",
      "Testing Data: (539, 22)\n"
     ]
    }
   ],
   "source": [
    "# Create holdout and testing datasets for 2013 data\n",
    "holdout_2013 = df[df['season'] == 2013]\n",
    "test_df_2013 = df[(df['season'] < 2013)]\n",
    "\n",
    "# Save both the holdout and validation set locally\n",
    "holdout_2013.to_csv('csv_files/holdouts/2013/holdout_data.csv')\n",
    "test_df_2013.to_csv('csv_files/holdouts/2013/test_df_data.csv')\n",
    "\n",
    "# Examine sizes\n",
    "print(f'Holdout Set: {holdout_2013.shape}')\n",
    "print(f'Testing Data: {test_df_2013.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout Set: (65, 22)\n",
      "Testing Data: (605, 22)\n"
     ]
    }
   ],
   "source": [
    "# Create holdout and testing datasets for 2014 data\n",
    "holdout_2014 = df[df['season'] == 2014]\n",
    "test_df_2014 = df[(df['season'] < 2014)]\n",
    "\n",
    "# Save both the holdout and validation set locally\n",
    "holdout_2014.to_csv('csv_files/holdouts/2014/holdout_data.csv')\n",
    "test_df_2014.to_csv('csv_files/holdouts/2014/test_df_data.csv')\n",
    "\n",
    "# Examine sizes\n",
    "print(f'Holdout Set: {holdout_2014.shape}')\n",
    "print(f'Testing Data: {test_df_2014.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout Set: (57, 22)\n",
      "Testing Data: (670, 22)\n"
     ]
    }
   ],
   "source": [
    "# Create holdout and testing datasets for 2015 data\n",
    "holdout_2015 = df[df['season'] == 2015]\n",
    "test_df_2015 = df[(df['season'] < 2015)]\n",
    "\n",
    "# Save both the holdout and validation set locally\n",
    "holdout_2015.to_csv('csv_files/holdouts/2015/holdout_data.csv')\n",
    "test_df_2015.to_csv('csv_files/holdouts/2015/test_df_data.csv')\n",
    "\n",
    "# Examine sizes\n",
    "print(f'Holdout Set: {holdout_2015.shape}')\n",
    "print(f'Testing Data: {test_df_2015.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout Set: (57, 22)\n",
      "Testing Data: (727, 22)\n"
     ]
    }
   ],
   "source": [
    "# Create holdout and testing datasets for 2016 data\n",
    "holdout_2016 = df[df['season'] == 2016]\n",
    "test_df_2016 = df[(df['season'] < 2016)]\n",
    "\n",
    "# Save both the holdout and validation set locally\n",
    "holdout_2016.to_csv('csv_files/holdouts/2016/holdout_data.csv')\n",
    "test_df_2016.to_csv('csv_files/holdouts/2016/test_df_data.csv')\n",
    "\n",
    "# Examine sizes\n",
    "print(f'Holdout Set: {holdout_2016.shape}')\n",
    "print(f'Testing Data: {test_df_2016.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout Set: (52, 22)\n",
      "Testing Data: (784, 22)\n"
     ]
    }
   ],
   "source": [
    "# Create holdout and testing datasets for 2017 data\n",
    "holdout_2017 = df[df['season'] == 2017]\n",
    "test_df_2017 = df[(df['season'] < 2017)]\n",
    "\n",
    "# Save both the holdout and validation set locally\n",
    "holdout_2017.to_csv('csv_files/holdouts/2017/holdout_data.csv')\n",
    "test_df_2017.to_csv('csv_files/holdouts/2017/test_df_data.csv')\n",
    "\n",
    "# Examine sizes\n",
    "print(f'Holdout Set: {holdout_2017.shape}')\n",
    "print(f'Testing Data: {test_df_2017.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout Set: (59, 22)\n",
      "Testing Data: (836, 22)\n"
     ]
    }
   ],
   "source": [
    "# Create holdout and testing datasets for 2018 data\n",
    "holdout_2018 = df[df['season'] == 2018]\n",
    "test_df_2018 = df[(df['season'] < 2018)]\n",
    "\n",
    "# Save both the holdout and validation set locally\n",
    "holdout_2018.to_csv('csv_files/holdouts/2018/holdout_data.csv')\n",
    "test_df_2018.to_csv('csv_files/holdouts/2018/test_df_data.csv')\n",
    "\n",
    "# Examine sizes\n",
    "print(f'Holdout Set: {holdout_2018.shape}')\n",
    "print(f'Testing Data: {test_df_2018.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout Set: (56, 22)\n",
      "Testing Data: (895, 22)\n"
     ]
    }
   ],
   "source": [
    "# Create holdout and testing datasets for 2019 data\n",
    "holdout_2019 = df[df['season'] == 2019]\n",
    "test_df_2019 = df[(df['season'] < 2019)]\n",
    "\n",
    "# Save both the holdout and validation set locally\n",
    "holdout_2019.to_csv('csv_files/holdouts/2019/holdout_data.csv')\n",
    "test_df_2019.to_csv('csv_files/holdouts/2019/test_df_data.csv')\n",
    "\n",
    "# Examine sizes\n",
    "print(f'Holdout Set: {holdout_2019.shape}')\n",
    "print(f'Testing Data: {test_df_2019.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv_files/holdouts/2019/test_df_data.csv', index_col=0)\n",
    "\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = df[df['top_10'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['season', 'champion', 'top_10',\n",
    "         'made_cut', 'total_score'],axis=1).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10.drop(['season', 'champion', 'top_10',\n",
    "         'made_cut', 'total_score'],axis=1).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Variable Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cut Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cutlines\n",
    "cut_lines = pd.read_csv('csv_files/masters/masters_results - cutline.csv')\n",
    "\n",
    "print(cut_lines.shape)\n",
    "cut_lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "target_variable = 'made_cut'\n",
    "title = target_variable.replace('_', ' ').title()\n",
    "fig = plt.figure(figsize=(13,5));\n",
    "\n",
    "# Percent of Golfers Cut\n",
    "height = df.groupby(target_variable).season.count().tolist()\n",
    "missed_cut = height[0]\n",
    "made_cut = height[1]\n",
    "ax1 = fig.add_subplot(121);\n",
    "ax1.bar(\n",
    "    x=0, \n",
    "    height=made_cut,\n",
    "    ec='darkgreen',\n",
    "    fc='darkgreen',\n",
    "    label=f'Made Cut: {made_cut/(made_cut+missed_cut) : 0.1%}'\n",
    ");\n",
    "ax1.bar(\n",
    "    x=1, \n",
    "    height=missed_cut,\n",
    "    ec='darkgreen',\n",
    "    fc='yellow',\n",
    "    label=f'Missed Cut: {missed_cut/(made_cut+missed_cut) : 0.1%}'\n",
    ");\n",
    "ax1.set_title(\"Percent of Golfers Cut\", {'fontsize' : 16});\n",
    "ax1.set_ylabel('Number of Golfers', {'fontsize' : 14});\n",
    "ax1.set_xticks(ticks=[0,1]);\n",
    "ax1.set_xticklabels(labels=['Made the Cut', 'Missed the Cut'], fontdict={'fontsize' : 14});\n",
    "ax1.legend(loc='upper right', fontsize='large');\n",
    "\n",
    "# Cutline overtime\n",
    "avg_cutline = round(cut_lines['relative_to_par'].mean(), 0)\n",
    "ax2 = fig.add_subplot(122);\n",
    "\n",
    "cut_lines.groupby('season').relative_to_par.mean().plot(\n",
    "    kind='line',\n",
    "    ax=ax2,\n",
    "    fontsize=12,\n",
    "    marker='o',\n",
    "    mec='darkgreen',\n",
    "    mfc='yellow',\n",
    "    linestyle='--',\n",
    "    linewidth=1.0,\n",
    "    color='red',\n",
    "    label=f'Avg. Cut Line: +{avg_cutline}'\n",
    ");\n",
    "ax2.set_title(\"Tournament Cut Line by Year\", {'fontsize' : 16});\n",
    "ax2.set_ylabel('Score Relative To Par', {'fontsize' : 14});\n",
    "ax2.set_yticks(range(0, 11));\n",
    "ax2.set_yticklabels(\n",
    "    ('E', '+1', '+2', '+3', '+4',\n",
    "     '+5', '+6', '+7', '+8', '+9', '+10')\n",
    ");\n",
    "ax2.set_xlim(2004, 2020);\n",
    "ax2.legend(loc='upper right', fontsize='large');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = 'top_10'\n",
    "title = target_variable.replace('_', ' ').title()\n",
    "fig = plt.figure(figsize=(13,5));\n",
    "\n",
    "# Percent of Golfers Inside Top 10\n",
    "height = df.groupby(target_variable).season.count().tolist()\n",
    "outside_top10 = height[0]\n",
    "inside_top10 = height[1]\n",
    "ax1 = fig.add_subplot(121);\n",
    "ax1.bar(\n",
    "    x=0, \n",
    "    height=inside_top10,\n",
    "    ec='darkgreen',\n",
    "    fc='darkgreen',\n",
    "    label=f'Inside: {inside_top10/(inside_top10+outside_top10) : 0.1%}'\n",
    ");\n",
    "ax1.bar(\n",
    "    x=1, \n",
    "    height=missed_cut,\n",
    "    ec='darkgreen',\n",
    "    fc='yellow',\n",
    "    label=f'Outside: {outside_top10/(inside_top10+outside_top10) : 0.1%}'\n",
    ");\n",
    "ax1.set_title(\"Top 10 Summary\", {'fontsize' : 16});\n",
    "ax1.set_ylabel('Number of Golfers', {'fontsize' : 14});\n",
    "ax1.set_xticks(ticks=[0,1]);\n",
    "ax1.set_xticklabels(labels=['Inside', 'Outside'], fontdict={'fontsize' : 14});\n",
    "ax1.legend(loc='upper left', fontsize='large');\n",
    "\n",
    "# Top 10 overtime\n",
    "top_10 = pd.DataFrame(df[df[target_variable] == 1].groupby('season').total_score.max())\n",
    "top_10.reset_index(drop=False, inplace=True)\n",
    "top_10['season'] = top_10['season']+1\n",
    "top_10['relative_to_par'] = top_10['total_score']-(72*4)\n",
    "avg_top10 = round(top_10['relative_to_par'].mean(), 0)\n",
    "ax2 = fig.add_subplot(122);\n",
    "\n",
    "top_10.groupby('season').relative_to_par.mean().plot(\n",
    "    kind='line',\n",
    "    ax=ax2,\n",
    "    fontsize=12,\n",
    "    marker='o',\n",
    "    mec='darkgreen',\n",
    "    mfc='yellow',\n",
    "    linestyle='--',\n",
    "    linewidth=1.0,\n",
    "    color='red',\n",
    "    label=f'Avg. Highest Top 10 Score: {avg_top10}'\n",
    ");\n",
    "ax2.set_title(\"Top 10 by Year\", {'fontsize' : 16});\n",
    "ax2.set_ylabel('Score Relative To Par', {'fontsize' : 14});\n",
    "ax2.legend(loc='upper right', fontsize='large');\n",
    "ax2.set_yticks(range(-11, 9));\n",
    "ax2.set_yticklabels(\n",
    "    ('-11', '-10', '-9', '-8',\n",
    "     '-7', '-6', '-5', '-4',\n",
    "     '-3', '-2', '-1', 'E',\n",
    "     '+1', '+2', '+3', '+4',\n",
    "     '+5', '+6', '+7', '+8')\n",
    ");\n",
    "ax2.set_xlim(2004, 2020);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = df[df['top_10'] == 1]\n",
    "top10 = pd.DataFrame(top10['full_name'].value_counts())\n",
    "top10.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Past Champions and Worst Performers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe that grabs golfers, their total experience, and number/pct of times they made the cut\n",
    "experience = pd.DataFrame(df.groupby('full_name').experience.max())\n",
    "experience.reset_index(drop=False, inplace=True)\n",
    "\n",
    "cut_summary = pd.DataFrame(df.groupby('full_name').made_cut.sum())\n",
    "cut_summary.reset_index(drop=False, inplace=True)\n",
    "\n",
    "cut_summary = pd.merge(experience, cut_summary, how='outer', on='full_name')\n",
    "cut_summary = cut_summary[['full_name', 'experience', 'made_cut']]\n",
    "cut_summary['cuts_missed'] = cut_summary['experience'] - cut_summary['made_cut']\n",
    "cut_summary['cuts_missed_pct'] = cut_summary['cuts_missed']/cut_summary['experience']*100\n",
    "cut_summary = cut_summary[cut_summary['experience'] >= 4]\n",
    "\n",
    "# Grab golfers that have missed more than 50% of cuts\n",
    "augusta_hates = cut_summary[cut_summary['cuts_missed_pct'] >= 50]\n",
    "raw_golfers = augusta_hates['full_name'].tolist()\n",
    "golfers = [golfer.replace('_', ' ').title() for golfer in raw_golfers]\n",
    "golfers = [golfer.replace('Iii', 'III') if golfer.endswith('Iii')\n",
    "           else golfer for golfer in golfers]\n",
    "augusta_hates['Golfers'] = golfers\n",
    "augusta_hates.set_index('Golfers', inplace=True)\n",
    "augusta_hates.sort_values(by='cuts_missed_pct', ascending=False, inplace=True)\n",
    "augusta_hates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13,5));\n",
    "ax = fig.add_subplot(111);\n",
    "augusta_hates['cuts_missed_pct'].plot.bar(\n",
    "    ax=ax,\n",
    "    ec='darkgreen',\n",
    "    fc='yellow',\n",
    "    fontsize=12\n",
    ");\n",
    "ax.set_title(\n",
    "    'Golfers That Miss the Cut Most Frequently\\n(minimum attemps = 4)', \n",
    "    {'fontsize' : 16}\n",
    ");\n",
    "ax.set_ylabel('Percent of Cuts Missed');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = 'champion'\n",
    "title = 'Masters Champion'\n",
    "fig = plt.figure(figsize=(13,5));\n",
    "\n",
    "#Champion's score overtime\n",
    "champion = pd.DataFrame(df[df[target_variable] == 1].groupby('season').total_score.max())\n",
    "champion.reset_index(drop=False, inplace=True)\n",
    "champion['season'] = champion['season']+1\n",
    "champion['relative_to_par'] = champion['total_score']-(72*4)\n",
    "avg_champion = round(champion['relative_to_par'].mean(), 0)\n",
    "ax2 = fig.add_subplot(122);\n",
    "\n",
    "champion.groupby('season').relative_to_par.mean().plot(\n",
    "    kind='line',\n",
    "    ax=ax2,\n",
    "    fontsize=12,\n",
    "    marker='o',\n",
    "    mec='darkgreen',\n",
    "    mfc='yellow',\n",
    "    linestyle='--',\n",
    "    linewidth=1.0,\n",
    "    color='red',\n",
    "    label=f'Masters Champion Avg. Score: {avg_champion}'\n",
    ");\n",
    "ax2.set_title(\"Champion's Score by Year\", {'fontsize' : 16});\n",
    "ax2.set_ylabel('Score Relative To Par', {'fontsize' : 14});\n",
    "ax2.legend(loc='upper right', fontsize='large');\n",
    "raw_champions = df[df['champion'] == 1].full_name.tolist()\n",
    "champions = [champion.replace('_', ' ').title() for champion in raw_champions]\n",
    "ax2.set_xticks(range(2005, 2020));\n",
    "ax2.set_xticklabels(champions, rotation=90);\n",
    "ax2.set_yticks(range(-20, 6, 2));\n",
    "ax2.set_yticklabels(\n",
    "    ('-20', '-18', '-16', '-14',\n",
    "     '-12', '-10', '-8', '-6',\n",
    "     '-4', '-2', 'E', '+2', '+4')\n",
    ");\n",
    "ax2.set_xlim(2004, 2020);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strokes Gained Over Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df[['season', 'full_name', 'experience', 'sg_roll_avg']]\n",
    "\n",
    "exp1 = dataset[(dataset['experience'] < 6)].sg_roll_avg.mean()\n",
    "exp2 = dataset[(dataset['experience'] > 5)&(dataset['experience'] < 11)].sg_roll_avg.mean()\n",
    "exp3 = dataset[(dataset['experience'] > 11)].sg_roll_avg.mean()\n",
    "\n",
    "print(exp1)\n",
    "print(exp2)\n",
    "print(exp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Strokes Gained on field by experience\n",
    "fig = plt.figure(figsize=figsize);\n",
    "ax = fig.add_subplot(111);\n",
    "\n",
    "\n",
    "sns.boxplot(\n",
    "    x=dataset['experience'],\n",
    "    y=dataset['sg_roll_avg']\n",
    ");\n",
    "ax.axvline(4.5, color='darkgray');\n",
    "ax.axvline(9.5, color='darkgray');\n",
    "ax.set_ylabel('Strokes Gained On Field', fontsize=14);\n",
    "ax.set_title('Strokes Gained On Field Rolling Average', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['champion', 'top_10', 'made_cut', 'total_score',\n",
    "            'rolling_majors', 'sg_roll_avg',\n",
    "            'driving_distance', 'driving_accuracy',\n",
    "            'greens_in_regulation', 'bob_distance',\n",
    "            'scrambling_rough', 'scrambling_sand', 'putting_rating',\n",
    "            'scoring_avg', 'sg_stat']\n",
    "\n",
    "df_corr = df[features]\n",
    "\n",
    "fig = plt.figure(figsize=(11,11))\n",
    "\n",
    "# Create mask\n",
    "mask = np.zeros_like(df_corr.corr(), dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Create color scheme\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "sns.heatmap(df_corr.corr(),\n",
    "            mask=mask, \n",
    "            cmap=cmap,\n",
    "            center=0,\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "            square=True,\n",
    "            linewidths=0.5,\n",
    "            cbar_kws={'shrink': 0.5},\n",
    "            ax=ax1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test For Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "fig.subplots_adjust(hspace=hspace);\n",
    "\n",
    "ax11 = fig.add_subplot(3,4,1);\n",
    "ax11.hist(df['rolling_majors'], fc = 'darkgreen');\n",
    "ax11.set_title('Rolling Majors');\n",
    "\n",
    "ax12 = fig.add_subplot(3,4,2);\n",
    "ax12.hist(df['sg_roll_avg'], fc='yellow', ec='darkgreen');\n",
    "ax12.set_title('SG on Field (RA)');\n",
    "\n",
    "ax13 = fig.add_subplot(3,4,3);\n",
    "ax13.hist(df['driving_distance'], fc = 'darkgreen');\n",
    "ax13.set_title('Driving Distance');\n",
    "\n",
    "ax14 = fig.add_subplot(3,4,4);\n",
    "ax14.hist(df['driving_accuracy'], fc='yellow', ec='darkgreen');\n",
    "ax14.set_title('Driving Accuracy');\n",
    "\n",
    "ax21 = fig.add_subplot(3,4,5);\n",
    "ax21.hist(df['greens_in_regulation'], fc='yellow', ec='darkgreen');\n",
    "ax21.set_title('GIR');\n",
    "\n",
    "ax22 = fig.add_subplot(3,4,6);\n",
    "ax22.hist(df['bob_distance'], fc = 'darkgreen');\n",
    "ax22.set_title('BoB Distance');\n",
    "\n",
    "ax23 = fig.add_subplot(3,4,7);\n",
    "ax23.hist(df['scrambling_rough'], fc='yellow', ec='darkgreen');\n",
    "ax23.set_title('Scrambling % (R)');\n",
    "\n",
    "ax24 = fig.add_subplot(3,4,8);\n",
    "ax24.hist(df['scrambling_sand'], fc = 'darkgreen');\n",
    "ax24.set_title('Scrambling % (S)');\n",
    "\n",
    "ax31 = fig.add_subplot(3,4,9);\n",
    "ax31.hist(df['putting_rating'], fc = 'darkgreen');\n",
    "ax31.set_title('Putting Rating');\n",
    "\n",
    "ax32 = fig.add_subplot(3,4,10);\n",
    "ax32.hist(df['scoring_avg'], fc='yellow', ec='darkgreen');\n",
    "ax32.set_title('Scoring Average');\n",
    "\n",
    "ax33 = fig.add_subplot(3,4,11);\n",
    "ax33.hist(df['scoring_avg'], fc = 'darkgreen');\n",
    "ax33.set_title('Strokes Gained: Stat');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop EDA Only Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define drop list\n",
    "drop = ['experience', 'prior_score', 'ps_roll_avg', 'prior_afs', 'sg_on_field']\n",
    "df = df.drop(drop, axis=1)\n",
    "\n",
    "# Examine new dataframe\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Cut Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Key Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random state\n",
    "SEED = 13\n",
    "\n",
    "# Split\n",
    "test_size = 0.2\n",
    "\n",
    "# Labels\n",
    "target_names = ['missed_cut', 'made_cut']\n",
    "\n",
    "FI_labels = df.drop(['full_name', 'season', 'champion',\n",
    "                     'top_10', 'made_cut', 'total_score'], axis=1).columns.values\n",
    "\n",
    "# K-folds Cross Validation\n",
    "cv = 3\n",
    "\n",
    "# Evaluation metric\n",
    "scoring = 'f1'\n",
    "\n",
    "# Train, Test, and Split\n",
    "X = df.drop(['full_name', 'season', 'champion',        \n",
    "             'top_10', 'made_cut', 'total_score'], axis=1)\n",
    "\n",
    "y = df['made_cut']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=SEED)\n",
    "\n",
    "# We will use smote to handle our class imbalance\n",
    "smt = SMOTE(random_state=SEED)\n",
    "X_train, y_train = smt.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Feature Summary')\n",
    "print(f'No. of Features: {len(X_train.columns.values)}')\n",
    "print(f'Features: {X_train.columns.values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Class Summary:')\n",
    "print('--'*20)\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate classifier and define model\n",
    "classifier = DummyClassifier(random_state = SEED)\n",
    "model_name = 'Dummy Classifier Model'\n",
    "\n",
    "# Create param grid for GridSearch\n",
    "param_grid = {'strategy' : ['uniform','stratified']}\n",
    "\n",
    "# Instantiate GridSearch\n",
    "dummy_clf = GridSearchCV(\n",
    "    classifier,\n",
    "    param_grid,\n",
    "    cv=cv,\n",
    "    scoring=scoring,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit and make predictions\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "y_hat_pred = dummy_clf.predict(X_train)\n",
    "y_pred = dummy_clf.predict(X_test)\n",
    "\n",
    "# Print Best Params\n",
    "print('--'*27)\n",
    "print(f'{model_name} Best Params:')\n",
    "print('--'*27)\n",
    "best_params = dummy_clf.best_params_\n",
    "keys = list(best_params.keys())\n",
    "for key in keys:\n",
    "    print(f\" {key} : {best_params[key]}\")\n",
    "\n",
    "# Print Precision Score\n",
    "f1_hat = metrics.f1_score(y_train, y_hat_pred)\n",
    "f1 = metrics.f1_score(y_test, y_pred)\n",
    "print('--'*27)\n",
    "print(f'{model_name} F1 Scores:')\n",
    "print('--'*27)\n",
    "print(f' train: {f1_hat : 0.2%}')\n",
    "print(f' test: {f1 : 0.2%}')\n",
    "\n",
    "# Print classification report\n",
    "print('--'*27)\n",
    "print(f'{model_name} Classification Report')\n",
    "print('--'*27)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=target_names))\n",
    "print('--'*27)\n",
    "\n",
    "# Print confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "classes = target_names\n",
    "plot_confusion_matrix(cm, classes)\n",
    "\n",
    "# Pickle model\n",
    "filename = 'models/classification/base_model.pkl'\n",
    "model = dummy_clf\n",
    "joblib.dump(model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate classifier and pipeline\n",
    "model_name = 'Logistic Regression Model'\n",
    "classifier = LogisticRegression(max_iter=1000, random_state=SEED)\n",
    "pipe = Pipeline(steps=[('pca', pca), ('classifier', classifier)])\n",
    "\n",
    "# Create param grid for GridSearch\n",
    "param_grid = {\n",
    "    'pca__n_components' :[3, 6, 11],\n",
    "    'classifier__penalty' : ['l1', 'l2'],\n",
    "    'classifier__solver' : ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "    'classifier__C' : np.logspace(-10,10,5),\n",
    "}\n",
    "\n",
    "# Instantiate GridSearch\n",
    "log_clf = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    cv=cv,\n",
    "    scoring=scoring,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit and make predictions\n",
    "log_clf.fit(X_train, y_train)\n",
    "y_hat_pred = log_clf.predict(X_train)\n",
    "y_pred = log_clf.predict(X_test)\n",
    "\n",
    "# Print Best Params\n",
    "print('--'*27)\n",
    "print(f'{model_name} Best Params:')\n",
    "print('--'*27)\n",
    "best_params = log_clf.best_params_\n",
    "keys = list(best_params.keys())\n",
    "for key in keys:\n",
    "    print(f\" {key} : {best_params[key]}\")\n",
    "\n",
    "# Print Precision Score\n",
    "f1_hat = metrics.f1_score(y_train, y_hat_pred)\n",
    "f1 = metrics.f1_score(y_test, y_pred)\n",
    "print('--'*27)\n",
    "print(f'{model_name} F1 Scores:')\n",
    "print('--'*27)\n",
    "print(f' train: {f1_hat : 0.2%}')\n",
    "print(f' test: {f1 : 0.2%}')\n",
    "\n",
    "# Print classification report\n",
    "print('--'*27)\n",
    "print(f'{model_name} Classification Report')\n",
    "print('--'*27)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=target_names))\n",
    "print('--'*27)\n",
    "\n",
    "# Print confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "classes = target_names\n",
    "plot_confusion_matrix(cm, classes)\n",
    "\n",
    "# Pickle model\n",
    "filename = 'models/classification/log_model.pkl'\n",
    "model = log_clf\n",
    "joblib.dump(model, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Logistic Regression Model\n",
    "log_optimized = LogisticRegression(\n",
    "    penalty = best_params['classifier__penalty'],\n",
    "    solver = best_params['classifier__solver'],\n",
    "    C = best_params['classifier__C']\n",
    ")\n",
    "\n",
    "log_optimized.fit(X_train, y_train)\n",
    "\n",
    "print(f'Intercept: {log_optimized.intercept_}')\n",
    "print(f'Coefficients: {log_optimized.coef_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate classifier and define model\n",
    "model_name = 'Decision Tree Model'\n",
    "classifier = DecisionTreeClassifier(random_state=SEED)\n",
    "pipe = Pipeline(steps=[('pca', pca), ('classifier', classifier)])\n",
    "\n",
    "# Create param grid for GridSearch\n",
    "param_grid = {\n",
    "    'pca__n_components' :[3, 6, 11],\n",
    "    'classifier__criterion' : ['gini', 'entropy'],\n",
    "    'classifier__max_depth' : [2, 5, 7],\n",
    "    'classifier__min_samples_split' : [5, 10],\n",
    "    'classifier__min_samples_leaf' : [5, 10]\n",
    "}\n",
    "\n",
    "# Instantiate GridSearch\n",
    "dt_clf = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    cv=cv,\n",
    "    scoring=scoring,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit and make predictions\n",
    "dt_clf.fit(X_train, y_train)\n",
    "y_hat_pred = dt_clf.predict(X_train)\n",
    "y_pred = dt_clf.predict(X_test)\n",
    "\n",
    "# Print Best Params\n",
    "print('--'*27)\n",
    "print(f'{model_name} Best Params:')\n",
    "print('--'*27)\n",
    "best_params = dt_clf.best_params_\n",
    "keys = list(best_params.keys())\n",
    "for key in keys:\n",
    "    print(f\" {key} : {best_params[key]}\")\n",
    "\n",
    "# Print Precision Score\n",
    "f1_hat = metrics.f1_score(y_train, y_hat_pred)\n",
    "f1 = metrics.f1_score(y_test, y_pred)\n",
    "print('--'*27)\n",
    "print(f'{model_name} F1 Scores:')\n",
    "print('--'*27)\n",
    "print(f' train: {f1_hat : 0.2%}')\n",
    "print(f' test: {f1 : 0.2%}')\n",
    "\n",
    "# Print classification report\n",
    "print('--'*27)\n",
    "print(f'{model_name} Classification Report')\n",
    "print('--'*27)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=target_names))\n",
    "print('--'*27)\n",
    "\n",
    "# Print confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "classes = target_names\n",
    "plot_confusion_matrix(cm, classes)\n",
    "\n",
    "# Feature importance\n",
    "dt_optimized = DecisionTreeClassifier(\n",
    "    criterion = best_params['classifier__criterion'],\n",
    "    max_depth = best_params['classifier__max_depth'],\n",
    "    min_samples_split = best_params['classifier__min_samples_split'],\n",
    "    min_samples_leaf = best_params['classifier__min_samples_leaf'],\n",
    "    random_state = SEED\n",
    ")\n",
    "\n",
    "dt_optimized.fit(X_train, y_train)\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "plot_feature_importances(X_train, dt_optimized, n_features, FI_labels)\n",
    "\n",
    "# Pickle model\n",
    "filename = 'models/classification/dt_model.pkl'\n",
    "model = dt_clf\n",
    "joblib.dump(model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate classifier and define model\n",
    "model_name = 'Random Forest Model'\n",
    "classifier = RandomForestClassifier(random_state=SEED)\n",
    "pipe = Pipeline(steps=[('pca', pca), ('classifier', classifier)])\n",
    "\n",
    "# Create param grid for GridSearch\n",
    "param_grid = {\n",
    "    'pca__n_components' :[11],\n",
    "    'classifier__criterion' : ['gini', 'entropy'],\n",
    "    'classifier__n_estimators' : [100],\n",
    "    'classifier__max_features' : ['auto'],\n",
    "    'classifier__max_depth' : [6],\n",
    "    'classifier__min_samples_split' : [6],\n",
    "    'classifier__min_samples_leaf' : [6]\n",
    "}\n",
    "\n",
    "# Instantiate GridSearch\n",
    "rf_clf = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    cv=cv,\n",
    "    scoring=scoring,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit and make predictions\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_hat_pred = rf_clf.predict(X_train)\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# Print Best Params\n",
    "print('--'*27)\n",
    "print(f'{model_name} Best Params:')\n",
    "print('--'*27)\n",
    "best_params = rf_clf.best_params_\n",
    "keys = list(best_params.keys())\n",
    "for key in keys:\n",
    "    print(f\" {key} : {best_params[key]}\")\n",
    "\n",
    "# Print Precision Score\n",
    "f1_hat = metrics.f1_score(y_train, y_hat_pred)\n",
    "f1 = metrics.f1_score(y_test, y_pred)\n",
    "print('--'*27)\n",
    "print(f'{model_name} F1 Scores:')\n",
    "print('--'*27)\n",
    "print(f' train: {f1_hat : 0.2%}')\n",
    "print(f' test: {f1 : 0.2%}')\n",
    "\n",
    "# Print classification report\n",
    "print('--'*27)\n",
    "print(f'{model_name} Classification Report')\n",
    "print('--'*27)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=target_names))\n",
    "print('--'*27)\n",
    "\n",
    "# Print confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "classes = target_names\n",
    "plot_confusion_matrix(cm, classes)\n",
    "\n",
    "# Feature importance\n",
    "rf_optimized = RandomForestClassifier(\n",
    "    criterion = best_params['classifier__criterion'],\n",
    "    n_estimators = best_params['classifier__n_estimators'],\n",
    "    max_features = best_params['classifier__max_features'],\n",
    "    max_depth = best_params['classifier__max_depth'],\n",
    "    min_samples_split = best_params['classifier__min_samples_split'],\n",
    "    min_samples_leaf = best_params['classifier__min_samples_leaf'],\n",
    "    random_state = SEED\n",
    ")\n",
    "\n",
    "rf_optimized.fit(X_train, y_train)\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "plot_feature_importances(X_train, rf_optimized, n_features, FI_labels)\n",
    "\n",
    "# Pickle model\n",
    "filename = 'models/classification/rf_model.pkl'\n",
    "model = rf_clf\n",
    "joblib.dump(model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Instantiate classifier and define model\n",
    "model_name = 'AdaBoost Model'\n",
    "classifier = AdaBoostClassifier(random_state=SEED)\n",
    "pipe = Pipeline(steps=[('pca', pca), ('classifier', classifier)])\n",
    "\n",
    "# Create param grid for GridSearch\n",
    "param_grid = {\n",
    "    'pca__n_components' : [3, 6, 11],\n",
    "    'classifier__n_estimators' : [150, 200],\n",
    "    'classifier__learning_rate' : [0.4, 0.5, 0.6],\n",
    "}\n",
    "\n",
    "# Instantiate GridSearch\n",
    "ab_clf = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    cv=cv,\n",
    "    scoring=scoring,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit and make predictions\n",
    "ab_clf.fit(X_train, y_train)\n",
    "y_hat_pred = ab_clf.predict(X_train)\n",
    "y_pred = ab_clf.predict(X_test)\n",
    "\n",
    "# Print Best Params\n",
    "print('--'*27)\n",
    "print(f'{model_name} Best Params:')\n",
    "print('--'*27)\n",
    "best_params = ab_clf.best_params_\n",
    "keys = list(best_params.keys())\n",
    "for key in keys:\n",
    "    print(f\" {key} : {best_params[key]}\")\n",
    "\n",
    "# Print Precision Score\n",
    "f1_hat = metrics.f1_score(y_train, y_hat_pred)\n",
    "f1 = metrics.f1_score(y_test, y_pred)\n",
    "print('--'*27)\n",
    "print(f'{model_name} F1 Scores:')\n",
    "print('--'*27)\n",
    "print(f' train: {f1_hat : 0.2%}')\n",
    "print(f' test: {f1 : 0.2%}')\n",
    "\n",
    "# Print classification report\n",
    "print('--'*27)\n",
    "print(f'{model_name} Classification Report')\n",
    "print('--'*27)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=target_names))\n",
    "print('--'*27)\n",
    "\n",
    "# Print confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "classes = target_names\n",
    "plot_confusion_matrix(cm, classes)\n",
    "\n",
    "# # Print Precision-Recall Curve\n",
    "# disp = metrics.plot_precision_recall_curve(ab_clf, X_test, y_test);\n",
    "\n",
    "# Feature importance\n",
    "ab_optimized = AdaBoostClassifier(\n",
    "    n_estimators = best_params['classifier__n_estimators'],\n",
    "    learning_rate = best_params['classifier__learning_rate'],\n",
    "    random_state = SEED\n",
    ")\n",
    "\n",
    "ab_optimized.fit(X_train, y_train)\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "plot_feature_importances(X_train, dt_optimized, n_features, FI_labels)\n",
    "\n",
    "# Pickle model\n",
    "filename = 'models/classification/ab_model.pkl'\n",
    "model = ab_clf\n",
    "joblib.dump(model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Instantiate classifier and define model\n",
    "model_name = 'Gradient Boost Model'\n",
    "classifier = GradientBoostingClassifier(random_state=SEED)\n",
    "pipe = Pipeline(steps=[('pca', pca), ('classifier', classifier)])\n",
    "\n",
    "# Create param grid for GridSearch\n",
    "param_grid = {\n",
    "    'pca__n_components' : [3, 6, 11],\n",
    "    'classifier__n_estimators' : [75, 100, 125],\n",
    "    'classifier__learning_rate' : [0.1, 0.2, 0.3],\n",
    "}\n",
    "\n",
    "# Instantiate GridSearch\n",
    "gb_clf = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    cv=cv,\n",
    "    scoring=scoring,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit and make predictions\n",
    "gb_clf.fit(X_train, y_train)\n",
    "y_hat_pred = gb_clf.predict(X_train)\n",
    "y_pred = gb_clf.predict(X_test)\n",
    "\n",
    "# Print Best Params\n",
    "print('--'*27)\n",
    "print(f'{model_name} Best Params:')\n",
    "print('--'*27)\n",
    "best_params = gb_clf.best_params_\n",
    "keys = list(best_params.keys())\n",
    "for key in keys:\n",
    "    print(f\" {key} : {best_params[key]}\")\n",
    "\n",
    "# Print Precision Score\n",
    "f1_hat = metrics.f1_score(y_train, y_hat_pred)\n",
    "f1 = metrics.f1_score(y_test, y_pred)\n",
    "print('--'*27)\n",
    "print(f'{model_name} F1 Scores:')\n",
    "print('--'*27)\n",
    "print(f' train: {f1_hat : 0.2%}')\n",
    "print(f' test: {f1 : 0.2%}')\n",
    "\n",
    "# Print classification report\n",
    "print('--'*27)\n",
    "print(f'{model_name} Classification Report')\n",
    "print('--'*27)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=target_names))\n",
    "print('--'*27)\n",
    "\n",
    "# Print confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "classes = target_names\n",
    "plot_confusion_matrix(cm, classes)\n",
    "\n",
    "# Feature importance\n",
    "gb_optimized = GradientBoostingClassifier(\n",
    "    n_estimators = best_params['classifier__n_estimators'],\n",
    "    learning_rate = best_params['classifier__learning_rate'],\n",
    "    random_state = SEED\n",
    ")\n",
    "\n",
    "gb_optimized.fit(X_train, y_train)\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "plot_feature_importances(X_train, gb_optimized, n_features, FI_labels)\n",
    "\n",
    "# Pickle model\n",
    "filename = 'models/classification/gb_model.pkl'\n",
    "model = gb_clf\n",
    "joblib.dump(model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [dummy_clf, log_clf, dt_clf, rf_clf, ab_clf, gb_clf]\n",
    "names = ['base', 'log_reg', 'decTree', 'randomF', 'adaB', 'gradB']\n",
    "class_results = []\n",
    "for index, classifier in enumerate(classifiers):\n",
    "    result = {}\n",
    "    y_hat_pred = classifier.predict(X_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    result['model'] = names[index]\n",
    "    result['accuracy'] = round(metrics.accuracy_score(y_test, y_pred)*100,2)\n",
    "    result['precision'] = round(metrics.precision_score(y_test, y_pred)*100,2)\n",
    "    result['recall'] = round(metrics.recall_score(y_test, y_pred)*100,2)\n",
    "    result['f1'] = round(metrics.f1_score(y_test, y_pred)*100,2)\n",
    "    class_results.append(result)\n",
    "\n",
    "class_results_df = pd.DataFrame(class_results)\n",
    "\n",
    "print(class_results_df.shape)\n",
    "class_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Score Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Key Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define drop list\n",
    "drop = ['full_name', 'season', 'champion', 'top_10', 'made_cut', 'total_score']\n",
    "\n",
    "# Train, test, split\n",
    "X = df.drop(drop, axis=1)\n",
    "y = df['total_score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dependent Variable Summary')\n",
    "print(f'No. of Dependent Variables: {len(X_train.columns.values)}')\n",
    "print(f'Dependent Variables: {X_train.columns.values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale Data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge\n",
    "model_name = 'Ridge Regression'\n",
    "classifier = Ridge()\n",
    "pipe = Pipeline(steps=[('pca', pca), ('classifier', classifier)])\n",
    "\n",
    "# Create param grid for GridSearch\n",
    "param_grid = {\n",
    "    'pca__n_components' : range(1, 12),\n",
    "    'classifier__alpha' : [0.1, 0.2, 0.5, 1]\n",
    "}\n",
    "\n",
    "# Instantiate GridSearch\n",
    "ridge = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    cv=cv,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit and make predictions\n",
    "ridge.fit(X_train, y_train)\n",
    "y_h_ridge_train = ridge.predict(X_train)\n",
    "y_h_ridge_test = ridge.predict(X_test)\n",
    "train_ridge_mse = np.sum((y_train - y_h_ridge_train)**2)/df.shape[0]\n",
    "train_ridge_r2 = metrics.r2_score(y_train, y_h_ridge_train)\n",
    "test_ridge_mse = np.sum((y_test - y_h_ridge_test)**2)/df.shape[0]\n",
    "test_ridge_r2 = metrics.r2_score(y_test, y_h_ridge_test)\n",
    "\n",
    "# Print Best Params\n",
    "print('--'*27)\n",
    "print(f'{model_name} Best Params:')\n",
    "print('--'*27)\n",
    "best_params = ridge.best_params_\n",
    "keys = list(best_params.keys())\n",
    "for key in keys:\n",
    "    print(f\" {key} : {best_params[key]}\")\n",
    "\n",
    "# Print rMSE and R2 results\n",
    "print('--'*27)\n",
    "print(f'{model_name} Error and Scores:')\n",
    "print('--'*27)\n",
    "print(f' Train rMSE: {round(np.sqrt(train_ridge_mse), 4)} | Train R2: {round(train_ridge_r2, 4)}')\n",
    "print(f' Test  rMSE: {round(np.sqrt(test_ridge_mse), 4)}  | Test  R2: {round(test_ridge_r2, 4)}')\n",
    "print('--'*27)\n",
    "\n",
    "# Pickle model\n",
    "filename = 'models/regression/ridge_model.pkl'\n",
    "model = ridge\n",
    "joblib.dump(model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso\n",
    "model_name = 'Lasso Regression'\n",
    "classifier = Lasso()\n",
    "pipe = Pipeline(steps=[('pca', pca), ('classifier', classifier)])\n",
    "\n",
    "# Create param grid for GridSearch\n",
    "param_grid = {\n",
    "    'pca__n_components' : range(1, 12),\n",
    "    'classifier__alpha' : [0.1, 0.2, 0.5, 1]\n",
    "}\n",
    "\n",
    "# Instantiate GridSearch\n",
    "lasso = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    cv=cv,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit and make predictions\n",
    "lasso.fit(X_train, y_train)\n",
    "y_h_lasso_train = lasso.predict(X_train)\n",
    "y_h_lasso_test = lasso.predict(X_test)\n",
    "train_lasso_mse = np.sum((y_train - y_h_lasso_train)**2)/df.shape[0]\n",
    "train_lasso_r2 = metrics.r2_score(y_train, y_h_lasso_train)\n",
    "test_lasso_mse = np.sum((y_test - y_h_lasso_test)**2)/df.shape[0]\n",
    "test_lasso_r2 = metrics.r2_score(y_test, y_h_lasso_test)\n",
    "\n",
    "# Print Best Params\n",
    "print('--'*27)\n",
    "print(f'{model_name} Best Params:')\n",
    "print('--'*27)\n",
    "best_params = lasso.best_params_\n",
    "keys = list(best_params.keys())\n",
    "for key in keys:\n",
    "    print(f\" {key} : {best_params[key]}\")\n",
    "\n",
    "# Print rMSE and R2 results\n",
    "print('--'*27)\n",
    "print(f'{model_name} Error and Scores:')\n",
    "print('--'*27)\n",
    "print(f' Train rMSE: {round(np.sqrt(train_lasso_mse), 4)} | Train R2: {round(train_lasso_r2, 4)}')\n",
    "print(f' Test  rMSE: {round(np.sqrt(test_lasso_mse), 4)}  | Test  R2: {round(test_lasso_r2, 4)}')\n",
    "print('--'*27)\n",
    "\n",
    "# Pickle model\n",
    "filename = 'models/regression/lasso_model.pkl'\n",
    "model = lasso\n",
    "joblib.dump(model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpenalized Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear\n",
    "model_name = 'Linear Regression'\n",
    "classifier = LinearRegression()\n",
    "pipe = Pipeline(steps=[('pca', pca), ('classifier', classifier)])\n",
    "\n",
    "# Create param grid for GridSearch\n",
    "param_grid = {'pca__n_components' : range(1, 12)}\n",
    "\n",
    "# Instantiate GridSearch\n",
    "linear = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    cv=cv,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit and make predictions\n",
    "linear.fit(X_train, y_train)\n",
    "y_h_linear_train = linear.predict(X_train)\n",
    "y_h_linear_test = linear.predict(X_test)\n",
    "train_linear_mse = np.sum((y_train - y_h_linear_train)**2)/df.shape[0]\n",
    "train_linear_r2 = metrics.r2_score(y_train, y_h_linear_train)\n",
    "test_linear_mse = np.sum((y_test - y_h_linear_test)**2)/df.shape[0]\n",
    "test_linear_r2 = metrics.r2_score(y_test, y_h_linear_test)\n",
    "\n",
    "# Print Best Params\n",
    "print('--'*27)\n",
    "print(f'{model_name} Best Params:')\n",
    "print('--'*27)\n",
    "best_params = linear.best_params_\n",
    "keys = list(best_params.keys())\n",
    "for key in keys:\n",
    "    print(f\" {key} : {best_params[key]}\")\n",
    "\n",
    "# Print rMSE and R2 results\n",
    "print('--'*27)\n",
    "print(f'{model_name} Error and Scores:')\n",
    "print('--'*27)\n",
    "print(f' Train rMSE: {round(np.sqrt(train_linear_mse), 4)} | Train R2: {round(train_linear_r2, 4)}')\n",
    "print(f' Test  rMSE: {round(np.sqrt(test_linear_mse), 4)}  | Test  R2: {round(test_linear_r2, 4)}')\n",
    "print('--'*27)\n",
    "\n",
    "# Pickle model\n",
    "filename = 'models/regression/linear_model.pkl'\n",
    "model = linear\n",
    "joblib.dump(model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "model_name = 'Gradient Boost Regression'\n",
    "classifier = HistGradientBoostingRegressor(loss='least_squares', random_state=SEED)\n",
    "pipe = Pipeline(steps=[('pca', pca), ('classifier', classifier)])\n",
    "\n",
    "# Create param grid for GridSearch\n",
    "param_grid = {\n",
    "    'pca__n_components' : [1, 2, 3],\n",
    "    'classifier__l2_regularization' : np.logspace(-5,5,5),\n",
    "    'classifier__max_depth' : [2, 5],\n",
    "    'classifier__learning_rate' : [0.1, 0.2, 0.5]\n",
    "}\n",
    "\n",
    "# Instantiate GridSearch\n",
    "gbr = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    cv=cv,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit and make predictions\n",
    "gbr.fit(X_train, y_train)\n",
    "y_h_gbr_train = gbr.predict(X_train)\n",
    "y_h_gbr_test = gbr.predict(X_test)\n",
    "train_gbr_mse = np.sum((y_train - y_h_gbr_train)**2)/df.shape[0]\n",
    "train_gbr_r2 = metrics.r2_score(y_train, y_h_gbr_train)\n",
    "test_gbr_mse = np.sum((y_test - y_h_gbr_test)**2)/df.shape[0]\n",
    "test_gbr_r2 = metrics.r2_score(y_test, y_h_gbr_test)\n",
    "\n",
    "# Print Best Params\n",
    "print('--'*27)\n",
    "print(f'{model_name} Best Params:')\n",
    "print('--'*27)\n",
    "best_params = gbr.best_params_\n",
    "keys = list(best_params.keys())\n",
    "for key in keys:\n",
    "    print(f\" {key} : {best_params[key]}\")\n",
    "\n",
    "# Print rMSE and R2 results\n",
    "print('--'*27)\n",
    "print(f'{model_name} Error and Scores:')\n",
    "print('--'*27)\n",
    "print(f' Train rMSE: {round(np.sqrt(train_gbr_mse), 4)} | Train R2: {round(train_gbr_r2, 4)}')\n",
    "print(f' Test  rMSE: {round(np.sqrt(test_gbr_mse), 4)}  | Test  R2: {round(test_gbr_r2, 4)}')\n",
    "print('--'*27)\n",
    "\n",
    "# Pickle model\n",
    "filename = 'models/regression/gbr_model.pkl'\n",
    "model = gbr\n",
    "joblib.dump(model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "model_name = 'Random Forest Regression'\n",
    "classifier = RandomForestRegressor(criterion='mse', random_state=SEED)\n",
    "pipe = Pipeline(steps=[('pca', pca), ('classifier', classifier)])\n",
    "\n",
    "# Create param grid for GridSearch\n",
    "param_grid = {\n",
    "    'pca__n_components' : [11],\n",
    "    'classifier__n_estimators' : [30, 40],\n",
    "    'classifier__max_features' : ['auto', 'sqrt', 'log2'],\n",
    "    'classifier__max_depth' : [2, 5],\n",
    "    'classifier__min_samples_split' : [25, 50],\n",
    "    'classifier__min_samples_leaf' : [25, 50]\n",
    "}\n",
    "\n",
    "# Instantiate GridSearch\n",
    "rfr = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    cv=cv,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit and make predictions\n",
    "rfr.fit(X_train, y_train)\n",
    "y_h_rfr_train = rfr.predict(X_train)\n",
    "y_h_rfr_test = rfr.predict(X_test)\n",
    "train_rfr_mse = np.sum((y_train - y_h_rfr_train)**2)/df.shape[0]\n",
    "train_rfr_r2 = metrics.r2_score(y_train, y_h_rfr_train)\n",
    "test_rfr_mse = np.sum((y_test - y_h_rfr_test)**2)/df.shape[0]\n",
    "test_rfr_r2 = metrics.r2_score(y_test, y_h_rfr_test)\n",
    "\n",
    "# Print Best Params\n",
    "print('--'*27)\n",
    "print(f'{model_name} Best Params:')\n",
    "print('--'*27)\n",
    "best_params = rfr.best_params_\n",
    "keys = list(best_params.keys())\n",
    "for key in keys:\n",
    "    print(f\" {key} : {best_params[key]}\")\n",
    "\n",
    "# Print rMSE and R2 results\n",
    "print('--'*27)\n",
    "print(f'{model_name} Error and Scores:')\n",
    "print('--'*27)\n",
    "print(f' Train rMSE: {round(np.sqrt(train_rfr_mse), 4)} | Train R2: {round(train_rfr_r2, 4)}')\n",
    "print(f' Test  rMSE: {round(np.sqrt(test_rfr_mse), 4)}  | Test  R2: {round(test_rfr_r2, 4)}')\n",
    "print('--'*27)\n",
    "\n",
    "# Pickle model\n",
    "filename = 'models/regression/rfr_model.pkl'\n",
    "model = rfr\n",
    "joblib.dump(model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [ridge, lasso, linear, rfr, gbr]\n",
    "names = ['ridge', 'lasso', 'unpenalized', 'randomF', 'gradB']\n",
    "reg_results = []\n",
    "for index, classifier in enumerate(classifiers):\n",
    "    result = {}\n",
    "    y_hat_pred = classifier.predict(X_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    result['model'] = names[index]\n",
    "    result['train_rmse'] = round(np.sqrt(np.sum((y_train - y_hat_pred)**2)/df.shape[0]), 4)\n",
    "    result['test_rmse'] = round(np.sqrt(np.sum((y_test - y_pred)**2)/df.shape[0]), 4)\n",
    "    reg_results.append(result)\n",
    "\n",
    "reg_results_df = pd.DataFrame(reg_results)\n",
    "\n",
    "print(reg_results_df.shape)\n",
    "reg_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "years = range(2012, 2020)\n",
    "rfr = RandomForestRegressor(\n",
    "    criterion = 'mse',\n",
    "    n_estimators = best_params['classifier__n_estimators'],\n",
    "    max_features = best_params['classifier__max_features'],\n",
    "    max_depth = best_params['classifier__max_depth'],\n",
    "    min_samples_split = best_params['classifier__min_samples_split'],\n",
    "    min_samples_leaf = best_params['classifier__min_samples_leaf'],\n",
    ")\n",
    "\n",
    "for year in years:\n",
    "    # Load holdout and test_df datasets\n",
    "    holdout = pd.read_csv(f'csv_files/holdouts/{year}/holdout_data.csv', index_col=0)\n",
    "    test_df = pd.read_csv(f'csv_files/holdouts/{year}/test_df_data.csv', index_col=0)\n",
    "\n",
    "    # Define X, y, and X_holdout\n",
    "    test_df.drop(['experience', 'prior_score', 'ps_roll_avg',\n",
    "                  'prior_afs', 'sg_on_field'], axis=1, inplace=True)\n",
    "    holdout.drop(['experience', 'prior_score', 'ps_roll_avg',\n",
    "                  'prior_afs', 'sg_on_field'], axis=1, inplace=True)\n",
    "\n",
    "    X = test_df.drop(['full_name', 'season', 'champion',  \n",
    "                      'top_10', 'made_cut', 'total_score'], axis=1)\n",
    "    y = test_df['made_cut']\n",
    "    X_holdout = holdout.drop(['full_name', 'season', 'champion',\n",
    "                              'top_10', 'made_cut', 'total_score'], axis=1)\n",
    "\n",
    "    # Refit classifier and predict which golfers will make the cut\n",
    "    rf_optimized = rf_optimized.fit(X, y)\n",
    "    cut_predictions = rf_optimized.predict(X_holdout)\n",
    "\n",
    "    # Define desired features\n",
    "    desired_features = ['full_name', 'rolling_majors', 'sg_roll_avg',\n",
    "                        'driving_distance', 'driving_accuracy',\n",
    "                        'greens_in_regulation', 'bob_distance',\n",
    "                        'scrambling_rough', 'scrambling_sand',\n",
    "                        'putting_rating', 'scoring_avg', 'sg_stat']\n",
    "\n",
    "    # Create new dataframe with features used in regression to predict if golferd final score\n",
    "    masters_field = pd.DataFrame(holdout[desired_features])\n",
    "\n",
    "    # Add predictions to the new dataframe\n",
    "    masters_field['made_cut'] = cut_predictions\n",
    "\n",
    "    # Define weekend_field, X, y, and X_weekend\n",
    "    weekend_field = masters_field[masters_field['made_cut'] == 1]\n",
    "    X = test_df.drop(['full_name', 'season', 'champion',\n",
    "                              'top_10', 'made_cut', 'total_score'], axis=1)\n",
    "    y = test_df['total_score']\n",
    "    X_weekend = weekend_field.drop(['full_name', 'made_cut'], axis=1)\n",
    "\n",
    "    # Standarize Data\n",
    "    X = scaler.fit_transform(X)\n",
    "    X_weekend = scaler.transform(X_weekend)\n",
    "\n",
    "    # Refit regression model and predict scores based on predicted weekend field\n",
    "    rfr = rfr.fit(X, y)\n",
    "    predictions = rfr.predict(X_weekend)\n",
    "    weekend_field['predicted_score'] = predictions\n",
    "    results = weekend_field[['full_name', 'predicted_score']]\n",
    "    results.sort_values(by='predicted_score', inplace=True)\n",
    "    results.to_csv(f'csv_files/holdouts/{year}/final_score_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2020 Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.iloc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back-Tested Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('csv_files/masters/masters_results - results.csv')\n",
    "dataset['accuracy'] = dataset['accuracy']*100\n",
    "print(dataset.shape)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.groupby('tournament_year').accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5));\n",
    "\n",
    "ax1 = fig.add_subplot(121);\n",
    "dataset.groupby('tournament_year').accuracy.mean().plot(\n",
    "    kind='line',\n",
    "    ax=ax1,\n",
    "    fontsize=12,\n",
    "    marker='o',\n",
    "    mec='darkgreen',\n",
    "    mfc='yellow',\n",
    "    linestyle='--',\n",
    "    linewidth=1.0,\n",
    "    color='red',\n",
    "    label='Prediction Accuracy'\n",
    ");\n",
    "x = (10/96)*100\n",
    "ax1.axhline(x, color='darkgreen', label='Top 10 Size (Hist. Avg.)');\n",
    "ax1.set_title('Accuracy of Top 10 Prediction', {'fontsize' : 16});\n",
    "ax1.set_ylabel('Percent', {'fontsize' : 14});\n",
    "ax1.set_xlim(2012, 2020);\n",
    "ax1.legend(loc='upper left', fontsize='large');\n",
    "\n",
    "# Percent of Golfers Cut\n",
    "height = dataset.groupby('winner_correct').tournament_year.count().tolist()\n",
    "incorrect = height[0]\n",
    "correct = height[1]\n",
    "ax2 = fig.add_subplot(122);\n",
    "ax2.bar(\n",
    "    x=0, \n",
    "    height=correct,\n",
    "    ec='darkgreen',\n",
    "    fc='darkgreen',\n",
    "    label=f'Correct: {correct/(correct+incorrect) : 0.1%}'\n",
    ");\n",
    "ax2.bar(\n",
    "    x=1, \n",
    "    height=incorrect,\n",
    "    ec='darkgreen',\n",
    "    fc='yellow',\n",
    "    label=f'Incorrect: {incorrect/(correct+incorrect) : 0.1%}'\n",
    ");\n",
    "ax2.set_title(\"Percent of Champions Predicted\", {'fontsize' : 16});\n",
    "ax2.set_ylabel('Number of Champions', {'fontsize' : 14});\n",
    "ax2.set_xticks(ticks=[0,1]);\n",
    "ax2.set_xticklabels(labels=['Correct', 'Incorrect'], fontdict={'fontsize' : 14});\n",
    "ax2.legend(loc='upper right', fontsize='large');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
